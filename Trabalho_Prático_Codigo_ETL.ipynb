{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "daee3fee-4e51-44d2-b363-32c370dbfbc7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nCollecting fastapi\n  Downloading fastapi-0.115.2-py3-none-any.whl (94 kB)\nCollecting uvicorn\n  Downloading uvicorn-0.32.0-py3-none-any.whl (63 kB)\nCollecting pyspark\n  Downloading pyspark-3.5.3.tar.gz (317.3 MB)\nRequirement already satisfied: requests in /databricks/python3/lib/python3.9/site-packages (2.27.1)\nCollecting apscheduler\n  Downloading APScheduler-3.10.4-py3-none-any.whl (59 kB)\nCollecting pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4\n  Downloading pydantic-2.9.2-py3-none-any.whl (434 kB)\nCollecting typing-extensions>=4.8.0\n  Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\nCollecting starlette<0.41.0,>=0.37.2\n  Downloading starlette-0.40.0-py3-none-any.whl (73 kB)\nCollecting h11>=0.8\n  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\nRequirement already satisfied: click>=7.0 in /databricks/python3/lib/python3.9/site-packages (from uvicorn) (8.0.4)\nCollecting py4j==0.10.9.7\n  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from requests) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.9/site-packages (from requests) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.9/site-packages (from requests) (2021.10.8)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /databricks/python3/lib/python3.9/site-packages (from requests) (1.26.9)\nRequirement already satisfied: pytz in /databricks/python3/lib/python3.9/site-packages (from apscheduler) (2021.3)\nCollecting tzlocal!=3.*,>=2.0\n  Downloading tzlocal-5.2-py3-none-any.whl (17 kB)\nRequirement already satisfied: six>=1.4.0 in /databricks/python3/lib/python3.9/site-packages (from apscheduler) (1.16.0)\nCollecting annotated-types>=0.6.0\n  Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\nCollecting pydantic-core==2.23.4\n  Downloading pydantic_core-2.23.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\nCollecting anyio<5,>=3.4.0\n  Downloading anyio-4.6.2.post1-py3-none-any.whl (90 kB)\nCollecting sniffio>=1.1\n  Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\nCollecting exceptiongroup>=1.0.2\n  Downloading exceptiongroup-1.2.2-py3-none-any.whl (16 kB)\nBuilding wheels for collected packages: pyspark\n  Building wheel for pyspark (setup.py): started\n  Building wheel for pyspark (setup.py): finished with status 'done'\n  Created wheel for pyspark: filename=pyspark-3.5.3-py2.py3-none-any.whl size=317840651 sha256=aff7e37240d8d48ecbd94e84be63946e1e427d783338a5592a55709d5478aeb4\n  Stored in directory: /root/.cache/pip/wheels/2e/d2/18/6f4f20e8332359f7fffceb6828edcc80ef96f86744192a7bb9\nSuccessfully built pyspark\nInstalling collected packages: typing-extensions, sniffio, exceptiongroup, pydantic-core, anyio, annotated-types, tzlocal, starlette, pydantic, py4j, h11, uvicorn, pyspark, fastapi, apscheduler\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing-extensions 4.1.1\n    Not uninstalling typing-extensions at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-bc0fc2b1-ba0a-40f6-af87-a5e34a3cd6bc\n    Can't uninstall 'typing-extensions'. No files were found to uninstall.\nSuccessfully installed annotated-types-0.7.0 anyio-4.6.2.post1 apscheduler-3.10.4 exceptiongroup-1.2.2 fastapi-0.115.2 h11-0.14.0 py4j-0.10.9.7 pydantic-2.9.2 pydantic-core-2.23.4 pyspark-3.5.3 sniffio-1.3.1 starlette-0.40.0 typing-extensions-4.12.2 tzlocal-5.2 uvicorn-0.32.0\nPython interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "%pip install fastapi uvicorn pyspark requests apscheduler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c79840b8-7477-4ba9-a55f-e0542fb63677",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Integração com automação de atualização a cada hora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9baa2b87-606f-4188-b964-1889ba91f88d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<command-936036379599824>:109: DeprecationWarning: \n        on_event is deprecated, use lifespan event handlers instead.\n\n        Read more about it in the\n        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).\n        \n  @app.on_event(\"startup\")\n<command-936036379599824>:116: DeprecationWarning: \n        on_event is deprecated, use lifespan event handlers instead.\n\n        Read more about it in the\n        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).\n        \n  @app.on_event(\"shutdown\")\nINFO:     Started server process [2264]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:50488 - \"GET /extradir_dados HTTP/1.1\" 404 Not Found\nINFO:     127.0.0.1:38946 - \"GET /extrair_dados HTTP/1.1\" 404 Not Found\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from fastapi import FastAPI, HTTPException\n",
    "import uvicorn\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "import requests\n",
    "import os\n",
    "from apscheduler.schedulers.background import BackgroundScheduler\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Configurando Spark\n",
    "spark = SparkSession.builder.appName(\"FastAPI Spark Parquet\").getOrCreate()\n",
    "\n",
    "# Definir o esquema dos artigos\n",
    "schema = StructType([\n",
    "    StructField(\"source\", StructType([\n",
    "        StructField(\"id\", StringType(), True),\n",
    "        StructField(\"name\", StringType(), True)\n",
    "    ]), True),\n",
    "    StructField(\"author\", StringType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"url\", StringType(), True),\n",
    "    StructField(\"urlToImage\", StringType(), True),\n",
    "    StructField(\"publishedAt\", StringType(), True),\n",
    "    StructField(\"content\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Definindo a API Key e o URL\n",
    "API_KEY = \"325474a96fcb4288bb357897851f51ce\"\n",
    "API_URL = \"https://newsapi.org/v2/everything\"\n",
    "PARQUET_PATH = \"/dbfs/FileStore/dados.parquet\"\n",
    "\n",
    "# Inicializando o agendador\n",
    "scheduler = BackgroundScheduler()\n",
    "\n",
    "def fetch_data():\n",
    "    \"\"\"\n",
    "    Função que faz a requisição à API e retorna os dados como DataFrame do Spark.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        'q': \"genomics or saúde or pesquisa\",  # Palavras-chave relevantes\n",
    "        'language': 'pt',  # Língua do artigo\n",
    "        'sortBy': 'publishedAt',  # Ordenar por data de publicação\n",
    "        'apiKey': API_KEY\n",
    "    }\n",
    "\n",
    "    # Fazer a requisição\n",
    "    response = requests.get(API_URL, params=params)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        raise HTTPException(status_code=response.status_code, detail=\"Erro ao acessar a API\")\n",
    "\n",
    "    data = response.json()\n",
    "    artigos = data['articles']\n",
    "\n",
    "    # Converter para DataFrame do Spark\n",
    "    df = spark.createDataFrame(artigos, schema=schema)\n",
    "    return df\n",
    "\n",
    "\n",
    "def update_parquet(df_atual, parquet_path):\n",
    "    \"\"\"\n",
    "    Função que verifica se o arquivo Parquet existe e realiza a atualização.\n",
    "    \"\"\"\n",
    "    # Verifica se o arquivo Parquet existe\n",
    "    try:\n",
    "        if os.path.exists(parquet_path):\n",
    "            # Se o arquivo existir, lê-lo\n",
    "            df_arquivo = spark.read.parquet(parquet_path)\n",
    "            print(\"Arquivo Parquet existente lido com sucesso.\")\n",
    "            \n",
    "            # Encontrar novos dados que não estão no Parquet\n",
    "            df_novos_dados = df_atual.join(df_arquivo, on=[\"title\"], how=\"left_anti\")\n",
    "            \n",
    "            if df_novos_dados.count() > 0:\n",
    "                # Gravar apenas novos dados\n",
    "                df_novos_dados.write.mode(\"append\").parquet(parquet_path)\n",
    "                return f\"{df_novos_dados.count()} novos registros adicionados ao Parquet.\"\n",
    "            else:\n",
    "                return \"Nenhum dado novo para atualizar.\"\n",
    "\n",
    "        else:\n",
    "            # Se o arquivo não existir, gravar df_atual como Parquet\n",
    "            df_atual.write.mode(\"overwrite\").parquet(parquet_path)\n",
    "            return \"Arquivo Parquet não existia. Criado novo arquivo.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Ocorreu um erro: {str(e)}\"\n",
    "\n",
    "\n",
    "def extrair_dados():\n",
    "    \"\"\"\n",
    "    Função que extrai dados da API e armazena em Parquet, se necessário.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extrair dados da API\n",
    "        df_atual = fetch_data()\n",
    "\n",
    "        # Atualizar o arquivo Parquet\n",
    "        resultado = update_parquet(df_atual, PARQUET_PATH)\n",
    "\n",
    "        return {\"message\": resultado}\n",
    "\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "\n",
    "@app.on_event(\"startup\")\n",
    "def start_scheduler():\n",
    "    # Agendar a tarefa para executar a extração de dados a cada hora\n",
    "    scheduler.add_job(extrair_dados, 'interval', hours=1)\n",
    "    scheduler.start()\n",
    "\n",
    "\n",
    "@app.on_event(\"shutdown\")\n",
    "def shutdown_scheduler():\n",
    "    scheduler.shutdown()\n",
    "\n",
    "\n",
    "@app.get(\"/\")\n",
    "def root():\n",
    "    return {\"message\": \"API de Atualização de Dados com FastAPI e Spark\"}\n",
    "\n",
    "\n",
    "# Executando o servidor\n",
    "if __name__ == \"__main__\":\n",
    "    config = uvicorn.Config(app)\n",
    "    server = uvicorn.Server(config)\n",
    "    await server.serve()    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7888bdd-fba1-49be-bacc-49bb4235079b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Integração com automação por requisição"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7aed3499-3fc3-4de5-b307-282d021097d1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "java.net.NoRouteToHostException: No route to host\n",
       "\tat java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n",
       "\tat java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.io.SelectorManager.doFinishConnect(SelectorManager.java:355)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector.processConnect(ManagedSelector.java:347)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector.access$1700(ManagedSelector.java:65)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector$SelectorProducer.processSelected(ManagedSelector.java:676)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector$SelectorProducer.produce(ManagedSelector.java:535)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produceTask(EatWhatYouKill.java:362)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:186)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\n",
       "\tat com.databricks.rpc.ShadedInstrumentedQueuedThreadPool$$anon$2.$anonfun$run$4(InstrumentedQueuedThreadPool.scala:183)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:273)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:269)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.rpc.ShadedInstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:130)\n",
       "\tat com.databricks.rpc.ShadedInstrumentedQueuedThreadPool$$anon$2.$anonfun$run$3(InstrumentedQueuedThreadPool.scala:183)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:126)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:123)\n",
       "\tat com.databricks.rpc.ShadedInstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:130)\n",
       "\tat com.databricks.rpc.ShadedInstrumentedQueuedThreadPool$$anon$2.run(InstrumentedQueuedThreadPool.scala:177)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\n",
       "\tat java.base/java.lang.Thread.run(Thread.java:829)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "java.net.NoRouteToHostException: No route to host\n\tat java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)\n\tat shaded.v9_4.org.eclipse.jetty.io.SelectorManager.doFinishConnect(SelectorManager.java:355)\n\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector.processConnect(ManagedSelector.java:347)\n\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector.access$1700(ManagedSelector.java:65)\n\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector$SelectorProducer.processSelected(ManagedSelector.java:676)\n\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector$SelectorProducer.produce(ManagedSelector.java:535)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produceTask(EatWhatYouKill.java:362)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:186)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\n\tat com.databricks.rpc.ShadedInstrumentedQueuedThreadPool$$anon$2.$anonfun$run$4(InstrumentedQueuedThreadPool.scala:183)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:273)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:269)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n\tat com.databricks.rpc.ShadedInstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:130)\n\tat com.databricks.rpc.ShadedInstrumentedQueuedThreadPool$$anon$2.$anonfun$run$3(InstrumentedQueuedThreadPool.scala:183)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:126)\n\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:123)\n\tat com.databricks.rpc.ShadedInstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:130)\n\tat com.databricks.rpc.ShadedInstrumentedQueuedThreadPool$$anon$2.run(InstrumentedQueuedThreadPool.scala:177)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
       "errorSummary": "Internal error. Attach your notebook to a different compute or restart the current compute.",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from fastapi import FastAPI, HTTPException\n",
    "import uvicorn\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "import requests\n",
    "import os\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Configurando Spark\n",
    "spark = SparkSession.builder.appName(\"FastAPI Spark Parquet\").getOrCreate()\n",
    "\n",
    "# Definir o esquema dos artigos\n",
    "schema = StructType([\n",
    "    StructField(\"source\", StructType([\n",
    "        StructField(\"id\", StringType(), True),\n",
    "        StructField(\"name\", StringType(), True)\n",
    "    ]), True),\n",
    "    StructField(\"author\", StringType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"url\", StringType(), True),\n",
    "    StructField(\"urlToImage\", StringType(), True),\n",
    "    StructField(\"publishedAt\", StringType(), True),\n",
    "    StructField(\"content\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Definindo a API Key e o URL\n",
    "API_KEY = \"325474a96fcb4288bb357897851f51ce\"\n",
    "API_URL = \"https://newsapi.org/v2/everything\"\n",
    "PARQUET_PATH = \"/dbfs/FileStore/dados.parquet\"\n",
    "\n",
    "def fetch_data():\n",
    "    \"\"\"\n",
    "    Função que faz a requisição à API e retorna os dados como DataFrame do Spark.\n",
    "    \"\"\"\n",
    "    # Parâmetros de consulta\n",
    "    params = {\n",
    "        'q': \"genomics OR genomic OR 'genetic sequencing' OR 'DNA research' OR 'genetic analysis'\",  # Palavras-chave relevantes\n",
    "        'language': 'pt',  # Língua do artigo\n",
    "        'sortBy': 'publishedAt',  # Ordenar por data de publicação\n",
    "        'apiKey': API_KEY\n",
    "    }\n",
    "\n",
    "    # Fazer a requisição\n",
    "    response = requests.get(API_URL, params=params)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        raise HTTPException(status_code=response.status_code, detail=\"Erro ao acessar a API\")\n",
    "\n",
    "    data = response.json()\n",
    "    artigos = data['articles']\n",
    "\n",
    "    # Converter para DataFrame do Spark\n",
    "    df = spark.createDataFrame(artigos, schema=schema)\n",
    "    return df\n",
    "\n",
    "def update_parquet(df_atual, parquet_path):\n",
    "    \"\"\"\n",
    "    Função que verifica se o arquivo Parquet existe e realiza a atualização.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Verifica se o arquivo Parquet existe usando dbutils\n",
    "        files = dbutils.fs.ls(os.path.dirname(parquet_path))\n",
    "        file_names = [file.name for file in files]\n",
    "\n",
    "        if os.path.basename(parquet_path) in file_names:\n",
    "            # Se o arquivo existir, lê-lo\n",
    "            df_arquivo = spark.read.parquet(parquet_path)\n",
    "            print(\"Arquivo Parquet existente lido com sucesso.\")\n",
    "            \n",
    "            # Encontrar novos dados que não estão no Parquet\n",
    "            df_novos_dados = df_atual.join(df_arquivo, on=[\"title\"], how=\"left_anti\")\n",
    "            \n",
    "            if df_novos_dados.count() > 0:\n",
    "                # Gravar apenas novos dados\n",
    "                df_novos_dados.write.mode(\"append\").parquet(parquet_path)\n",
    "                return f\"{df_novos_dados.count()} novos registros adicionados ao Parquet.\"\n",
    "            else:\n",
    "                return \"Nenhum dado novo para atualizar.\"\n",
    "\n",
    "        else:\n",
    "            # Se o arquivo não existir, gravar df_atual como Parquet\n",
    "            df_atual.write.mode(\"overwrite\").parquet(parquet_path)\n",
    "            return \"Arquivo Parquet não existia. Criado novo arquivo.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Ocorreu um erro: {str(e)}\"\n",
    "\n",
    "@app.get(\"/extrair_dados\")\n",
    "def extrair_dados():\n",
    "    \"\"\"\n",
    "    Endpoint para extrair dados da API e armazenar em Parquet, se necessário.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extrair dados da API\n",
    "        df_atual = fetch_data()\n",
    "\n",
    "        # Atualizar o arquivo Parquet\n",
    "        resultado = update_parquet(df_atual, PARQUET_PATH)\n",
    "\n",
    "        return {\"message\": resultado}\n",
    "\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.get(\"/\")\n",
    "def root():\n",
    "    return {\"message\": \"API de Atualização de Dados com FastAPI e Spark\"}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = uvicorn.Config(app)\n",
    "    server = uvicorn.Server(config)\n",
    "    await server.serve()    \n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Trabalho_Prático_Codigo_ETL",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
