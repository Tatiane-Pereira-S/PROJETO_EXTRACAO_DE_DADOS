{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca9ea387-d7c8-4a8f-b9a4-4f89ecf31f65",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Análise de Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f5285bb-9b01-4e5d-bd2b-05dbbe86cbc8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09647fab-8e87-438c-829b-1ce23c50c90b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nCollecting fastapi\n  Using cached fastapi-0.115.2-py3-none-any.whl (94 kB)\nCollecting uvicorn\n  Using cached uvicorn-0.32.0-py3-none-any.whl (63 kB)\nCollecting pyspark\n  Using cached pyspark-3.5.3-py2.py3-none-any.whl\nRequirement already satisfied: requests in /databricks/python3/lib/python3.9/site-packages (2.27.1)\nCollecting pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4\n  Using cached pydantic-2.9.2-py3-none-any.whl (434 kB)\nCollecting typing-extensions>=4.8.0\n  Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\nCollecting starlette<0.41.0,>=0.37.2\n  Using cached starlette-0.40.0-py3-none-any.whl (73 kB)\nCollecting h11>=0.8\n  Using cached h11-0.14.0-py3-none-any.whl (58 kB)\nRequirement already satisfied: click>=7.0 in /databricks/python3/lib/python3.9/site-packages (from uvicorn) (8.0.4)\nCollecting py4j==0.10.9.7\n  Using cached py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from requests) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.9/site-packages (from requests) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.9/site-packages (from requests) (2021.10.8)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /databricks/python3/lib/python3.9/site-packages (from requests) (1.26.9)\nCollecting annotated-types>=0.6.0\n  Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\nCollecting pydantic-core==2.23.4\n  Using cached pydantic_core-2.23.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\nCollecting anyio<5,>=3.4.0\n  Using cached anyio-4.6.2.post1-py3-none-any.whl (90 kB)\nCollecting sniffio>=1.1\n  Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\nCollecting exceptiongroup>=1.0.2\n  Using cached exceptiongroup-1.2.2-py3-none-any.whl (16 kB)\nInstalling collected packages: typing-extensions, sniffio, exceptiongroup, pydantic-core, anyio, annotated-types, starlette, pydantic, py4j, h11, uvicorn, pyspark, fastapi\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing-extensions 4.1.1\n    Not uninstalling typing-extensions at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-49c4092c-d63e-4444-bb4f-6113accdfe4c\n    Can't uninstall 'typing-extensions'. No files were found to uninstall.\nSuccessfully installed annotated-types-0.7.0 anyio-4.6.2.post1 exceptiongroup-1.2.2 fastapi-0.115.2 h11-0.14.0 py4j-0.10.9.7 pydantic-2.9.2 pydantic-core-2.23.4 pyspark-3.5.3 sniffio-1.3.1 starlette-0.40.0 typing-extensions-4.12.2 uvicorn-0.32.0\nPython interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "%pip install fastapi uvicorn pyspark requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "882beb8d-c231-4716-bdfe-2789d31096c5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Realização de Requisição"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "293a2428-e301-477e-8cea-beb191cedc98",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'message': 'Arquivo Parquet não existia. Criado novo arquivo.'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "# URL que você deseja acessar\n",
    "url = \"http://127.0.0.1:8000/extrair_dados\"\n",
    "\n",
    "# Fazendo a requisição GET\n",
    "response = requests.get(url)\n",
    "\n",
    "# Verificando o status da resposta\n",
    "if response.status_code == 200:\n",
    "    # A requisição foi bem-sucedida, você pode acessar o conteúdo\n",
    "    data = response.json()  # Supondo que a resposta seja em JSON\n",
    "    print(data)\n",
    "else:\n",
    "    # A requisição falhou, você pode tratar o erro\n",
    "    print(f\"Erro ao acessar a URL: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4fc1af9e-b0cc-46bf-bb1b-49ee3388e708",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Análises Solicitadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1348814c-4d91-4094-a185-1753375bd4f1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados transformados e armazenados com sucesso.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, year, month, dayofmonth, lower\n",
    "\n",
    "# Configurando Spark\n",
    "spark = SparkSession.builder.appName(\"TransformacoesDados\").getOrCreate()\n",
    "\n",
    "def transform_data(parquet_path):\n",
    "    \"\"\"\n",
    "    Função para ler dados do Parquet e aplicar transformações.\n",
    "    \"\"\"\n",
    "    # Ler o arquivo Parquet\n",
    "    df = spark.read.parquet(parquet_path)\n",
    "\n",
    "    # 4.1 - Quantidade de notícias por ano, mês e dia de publicação\n",
    "    df_data = df.groupBy(\n",
    "        year(\"publishedAt\").alias(\"ano\"),\n",
    "        month(\"publishedAt\").alias(\"mes\"),\n",
    "        dayofmonth(\"publishedAt\").alias(\"dia\")\n",
    "    ).agg(count(\"*\").alias(\"quantidade_noticias\"))\n",
    "\n",
    "    df_data.write.mode(\"overwrite\").parquet(\"/dbfs/FileStore/quantidade_noticias_por_data.parquet\")\n",
    "\n",
    "    # 4.2 - Quantidade de notícias por fonte e autor\n",
    "    df_autor_fonte = df.groupBy(\n",
    "        col(\"source.name\").alias(\"fonte\"),\n",
    "        col(\"author\").alias(\"autor\")\n",
    "    ).agg(count(\"*\").alias(\"quantidade_noticias\"))\n",
    "\n",
    "    df_autor_fonte.write.mode(\"overwrite\").parquet(\"/dbfs/FileStore/quantidade_noticias_por_fonte_autor.parquet\")\n",
    "\n",
    "    # 4.3 - Quantidade de aparições de 3 palavras chaves por ano, mês e dia\n",
    "    palavras_chaves = [\"genomics\",\"genomic\",\"genetic sequencing\",\"DNA research\",\"genetic analysis\"]  # Altere para as suas palavras-chave\n",
    "\n",
    "    # Filtrar o DataFrame com base nas palavras-chave\n",
    "    df_keywords = df.filter(lower(col(\"title\")).contains(palavras_chaves[0]) |\n",
    "                            lower(col(\"title\")).contains(palavras_chaves[1]) |\n",
    "                            lower(col(\"title\")).contains(palavras_chaves[2]))\n",
    "\n",
    "    df_keywords_data = df_keywords.groupBy(\n",
    "        year(\"publishedAt\").alias(\"ano\"),\n",
    "        month(\"publishedAt\").alias(\"mes\"),\n",
    "        dayofmonth(\"publishedAt\").alias(\"dia\")\n",
    "    ).agg(count(\"*\").alias(\"quantidade_aparicoes\"))\n",
    "\n",
    "    df_keywords_data.write.mode(\"overwrite\").parquet(\"/dbfs/FileStore/quantidade_aparicoes_palavras_chaves.parquet\")\n",
    "\n",
    "    print(\"Dados transformados e armazenados com sucesso.\")\n",
    "\n",
    "# Definir o caminho do Parquet\n",
    "PARQUET_PATH = \"/dbfs/FileStore/dados.parquet\"\n",
    "\n",
    "# Executar a transformação\n",
    "transform_data(PARQUET_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f826944-daf2-48a0-9e14-bba05816e46a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados de quantidade de notícias por data:\n+----+---+---+-------------------+\n|ano |mes|dia|quantidade_noticias|\n+----+---+---+-------------------+\n|2024|9  |23 |2                  |\n|2024|9  |21 |1                  |\n|2024|9  |25 |2                  |\n|2024|9  |26 |2                  |\n|2024|10 |12 |1                  |\n|2024|10 |11 |3                  |\n|2024|10 |14 |3                  |\n|2024|10 |15 |2                  |\n|2024|10 |18 |5                  |\n|2024|10 |10 |3                  |\n|2024|10 |2  |2                  |\n|2024|10 |8  |2                  |\n|2024|10 |9  |1                  |\n|2024|10 |17 |2                  |\n|2024|10 |16 |1                  |\n|2024|9  |29 |1                  |\n|2024|10 |1  |1                  |\n|2024|9  |30 |1                  |\n+----+---+---+-------------------+\n\nNúmero de registros: 18\n\nDados de quantidade de notícias por fonte e autor:\n+------------------------+-------------------------------+-------------------+\n|fonte                   |autor                          |quantidade_noticias|\n+------------------------+-------------------------------+-------------------+\n|Startupi.com.br         |Marystela Barbosa              |1                  |\n|Terra.com.br            |Redação Byte                   |1                  |\n|Uol.com.br              |null                           |1                  |\n|Suno.com.br             |Suno                           |1                  |\n|Conjur.com.br           |Roberto Duque Estrada          |1                  |\n|Uol.com.br              |Agência Fapesp                 |1                  |\n|Conjur.com.br           |Fernando Capez                 |1                  |\n|Observador.pt           |Observador                     |1                  |\n|Catracalivre.com.br     |Márcio Diniz                   |2                  |\n|Abertoatedemadrugada.com|Carlos Martins                 |2                  |\n|Conjur.com.br           |David Pimentel Barbosa de Siena|1                  |\n|Abril.com.br            |Diogo Sponchiato               |2                  |\n|Terra.com.br            |Porto Alegre 24 horas          |2                  |\n|Uxdesign.cc             |Pedro Pimentel                 |1                  |\n|Investing.com           |Investing.com                  |1                  |\n|Abril.com.br            |Bruno Carbinatto               |1                  |\n|Observador.pt           |Agência Lusa                   |1                  |\n|Perfil.com              |Perfil Brasil                  |1                  |\n|Conjur.com.br           |Demóstenes Torres              |1                  |\n|Tecmundo.com.br         |Igor Almenara Carneiro         |1                  |\n+------------------------+-------------------------------+-------------------+\nonly showing top 20 rows\n\nNúmero de registros: 27\n\nDados de aparições de palavras-chave por data:\n+---+---+---+--------------------+\n|ano|mes|dia|quantidade_aparicoes|\n+---+---+---+--------------------+\n+---+---+---+--------------------+\n\nNúmero de registros: 0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Configurando Spark\n",
    "spark = SparkSession.builder.appName(\"LerDadosTransformados\").getOrCreate()\n",
    "\n",
    "def read_and_print_parquet(parquet_path):\n",
    "    \"\"\"\n",
    "    Função para ler um arquivo Parquet e imprimir seus dados.\n",
    "    \"\"\"\n",
    "    # Ler o arquivo Parquet\n",
    "    df = spark.read.parquet(parquet_path)\n",
    "\n",
    "    # Mostrar os dados\n",
    "    df.show(truncate=False)\n",
    "    print(f\"Número de registros: {df.count()}\")\n",
    "\n",
    "# Definindo os caminhos dos novos arquivos Parquet\n",
    "parquet_noticias_por_data = \"/dbfs/FileStore/quantidade_noticias_por_data.parquet\"\n",
    "parquet_noticias_por_fonte_autor = \"/dbfs/FileStore/quantidade_noticias_por_fonte_autor.parquet\"\n",
    "parquet_aparicoes_palavras_chaves = \"/dbfs/FileStore/quantidade_aparicoes_palavras_chaves.parquet\"\n",
    "\n",
    "# Lendo e imprimindo os dados dos arquivos Parquet\n",
    "print(\"Dados de quantidade de notícias por data:\")\n",
    "read_and_print_parquet(parquet_noticias_por_data)\n",
    "\n",
    "print(\"\\nDados de quantidade de notícias por fonte e autor:\")\n",
    "read_and_print_parquet(parquet_noticias_por_fonte_autor)\n",
    "\n",
    "print(\"\\nDados de aparições de palavras-chave por data:\")\n",
    "read_and_print_parquet(parquet_aparicoes_palavras_chaves)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Análises",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
